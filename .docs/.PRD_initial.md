## Introduction

This document describes a demonstration chat application that helps researchers upload and interact with PDF documents. The focus is on minimal setup and ease of deployment rather than production-level features. It leverages a combination of FastAPI, Pydantic AI, Pinecone, Streamlit, and OpenAI's embedding model to create a lightweight research assistant.

## Assumptions

- **No authentication or accounts**: The app does not require user login or accounts.
- **In-memory processing**: PDFs are processed in-memory (no need for persistent file storage).
- **OpenAI Embeddings**: The app uses OpenAI's text-embedding-ada-002 model for generating text embeddings.
- **Not built for scale**: It's not optimized for very large documents or many concurrent users (just a demo).
- **Minimal error handling**: Basic error checks only, since this is a proof-of-concept.
- **Local deployment**: Assumes the app runs locally or on a simple cloud service (e.g. Streamlit Sharing, Render).

## Objectives

- **PDF Upload & Query**: Enable users to upload PDF files and ask questions about their content.
- **Real-time Text Extraction**: Efficiently extract text from PDFs upon upload for immediate use.
- **Embedding & Retrieval**: Generate embeddings for text chunks and store them in a vector database for fast similarity search.
- **Interactive UI**: Provide an intuitive front-end (via Streamlit) for chatting with the document content.
- **Minimal Setup**: Keep the installation and configuration simple for demonstration purposes.

## System Architecture

The system consists of five main components working together:

### Frontend (Streamlit UI)
- **Interface**: A Streamlit web app provides an interface to upload PDF files and enter queries.
- **User Interaction**: It sends the uploaded file to the backend for processing and user questions to the backend API.
- **Display**: Shows the answers or relevant text snippets returned by the backend, enabling an interactive chat experience.

### Backend API (FastAPI)
- **API Endpoints**: FastAPI serves endpoints for uploading PDFs and submitting queries.
- **Processing Coordination**: Receives the PDF from the frontend, triggers text extraction, and handles query processing.
- **Response Handling**: Sends back the query results (answers or relevant context) to the frontend UI.

### AI Processing (Pydantic AI & OpenAI Embeddings)
- **Embeddings Generation**: Uses OpenAI's text-embedding-ada-002 model to convert text chunks into numerical vector embeddings.
- **Structured Query Handling**: Utilizes Pydantic AI to structure the interaction between the user's query and the document context. Pydantic AI helps define expected response formats and validate data, making the AI integration more reliable.

### Vector Database (Pinecone)
- **Storage of Embeddings**: Uses Pinecone's vector database to store the embeddings of PDF text chunks.
- **Similarity Search**: On each query, the system finds which stored text chunks are most relevant by performing a similarity search.
- **Fast Retrieval**: Pinecone returns the relevant text embeddings (and their original text) quickly, enabling the app to retrieve supporting passages in milliseconds.

### Text Extraction (PyMuPDF)
- **PDF Parsing**: Utilizes PyMuPDF to extract textual content from PDF files while preserving basic structure.
- **Text Chunking**: Splits the extracted text into manageable chunks (e.g. chunks of 512–1024 tokens).
- **Real-time Efficiency**: Extraction and splitting are done on-the-fly when a PDF is uploaded.

## Implementation Details

### File Structure

The project is organized into frontend and backend directories, with a configuration folder for settings:

```
research_Agent_RAG/
│
├── backend/
│   ├── main.py             # FastAPI app entry point and route definitions
│   ├── pdf_processing.py   # Functions for text extraction from PDFs (using PyMuPDF)
│   ├── embeddings.py       # Functions to call OpenAI API for generating embeddings
│   ├── vector_store.py     # Functions to interact with Pinecone (store/retrieve vectors)
│   └── query_handler.py    # Logic to handle user queries and formulate responses
│
├── frontend/
│   ├── app.py              # Streamlit app entry point (UI logic)
│   └── components.py    # Helper components for UI layout (buttons, text boxes, etc.)
│       ├── __init__.py
│       ├── pdf_upload.py
│       ├── document_details.py
│       ├── chat_interface.py
│       └── document_preview.py
│
├── .env                    # Configuration (API keys for OpenAI/Pinecone, model settings)
│
├── requirements.txt        # Python dependencies
├── .gitignore              # 
└── README.md               # Project documentation and usage instructions
```

### Component Details

#### Backend (FastAPI)
- **main.py**: Initializes the FastAPI app and defines API endpoints.
- **pdf_processing.py**: Contains functions to extract text from PDF files using PyMuPDF.
- **embeddings.py**: Contains utility functions to call the OpenAI API for embeddings.
- **vector_store.py**: Manages storing and retrieving embeddings in Pinecone.
- **query_handler.py**: Orchestrates the process of answering a user's query.

#### Frontend (Streamlit UI)
- **app.py**: The main script for the Streamlit app that sets up the web interface elements.
- **ui_components.py**: Defines reusable UI elements to keep app.py clean.

### Data Flow

The interaction between components happens in the following sequence:

1. **PDF Upload (Frontend)**: The user selects a PDF file in the Streamlit app and uploads it.
2. **Backend Receives File**: The Streamlit app sends the PDF file to a FastAPI endpoint.
3. **Text Extraction**: The backend extracts text from the PDF and splits it into chunks.
4. **Embedding Creation**: For each text chunk, embeddings are generated using OpenAI's API.
5. **Store in Pinecone**: The embeddings are inserted into a Pinecone index with identifiers linking to original text.
6. **User Query (Frontend)**: The user enters a question about the PDF's content.
7. **Retrieve Relevant Chunks**: The query is processed to find the most similar stored embeddings.
8. **Formulate Answer**: The query handler uses Pydantic AI to formulate a response based on relevant text chunks.
9. **Display Answer**: The Streamlit app receives and displays the answer in the chat interface.

*Note: Steps 1–5 typically happen once per uploaded document. Steps 6–9 happen each time the user asks a question.*


## Technical Stack

- **Frontend**: Streamlit – for quickly building an interactive web UI in Python
- **Backend**: FastAPI – to create a lightweight REST API
- **Embeddings Model**: OpenAI's text-embedding-ada-002
- **Vector Database**: Pinecone – to store and query embeddings efficiently
- **PDF Text Extraction**: PyMuPDF (also known as fitz)
- **Orchestration/AI Framework**: Pydantic AI – for structured inputs/outputs

## Integration Points

- **Streamlit & FastAPI**: Communication via HTTP requests
- **FastAPI & OpenAI API**: Backend calls OpenAI's embedding API
- **FastAPI & Pinecone**: Backend uses Pinecone's Python client or REST API
- **Pydantic AI & LLM**: Query handler integrates with an LLM for structured outputs

## Development Roadmap

Development is planned in several phases:

### Phase 1: Project Setup
Initialize project structure, set up virtual environment, and install required libraries.

### Phase 2: PDF Upload & Text Extraction
Implement file upload in Streamlit UI and the backend endpoint for PDF processing.

### Phase 3: Embeddings & Pinecone Integration
Implement embedding generation and storage in Pinecone index.

### Phase 4: Query Handling & AI Response
Develop the query workflow for generating embeddings, querying Pinecone, and formatting answers.

### Phase 5: Refine UI & Testing
Polish the interface and test the entire flow end-to-end with various PDFs.

Throughout development, the focus remains on core functionality rather than edge cases or complex features, in line with the demonstration goal.
